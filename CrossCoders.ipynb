{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 15:35:14.643298: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-13 15:35:14.783007: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-13 15:35:14.787506: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2025-03-13 15:35:14.787517: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2025-03-13 15:35:14.811859: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-13 15:35:15.345639: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2025-03-13 15:35:15.345688: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2025-03-13 15:35:15.345694: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pprint\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Union\n",
    "from huggingface_hub import hf_hub_download, notebook_login\n",
    "import json\n",
    "import einops\n",
    "from typing import NamedTuple\n",
    "from transformer_lens import HookedTransformer\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from transformers import GPT2Tokenizer\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-medium into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model stanford-gpt2-medium-a into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "torch.set_grad_enabled(False) # important for memory\n",
    "\n",
    "base_model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-medium\",\n",
    "    device=device,\n",
    "    dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "chat_model = HookedTransformer.from_pretrained(\n",
    "    \"stanford-gpt2-medium-a\",\n",
    "    device=device,\n",
    "    dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from transformer_lens import ActivationCache\n",
    "import numpy as np \n",
    "\n",
    "DTYPES = {\"fp32\": torch.float32, \"fp16\": torch.float16, \"bf16\": torch.bfloat16}\n",
    "\n",
    "class Buffer:\n",
    "    \"\"\"\n",
    "    This defines a data buffer, to store a stack of acts across both model that can be used to train the autoencoder. It'll automatically run the model to generate more when it gets halfway empty.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg, model_A, model_B, all_tokens):\n",
    "        assert model_A.cfg.d_model == model_B.cfg.d_model\n",
    "        self.cfg = cfg\n",
    "        self.buffer_size = cfg[\"batch_size\"] * cfg[\"buffer_mult\"]\n",
    "        self.buffer_batches = self.buffer_size // (cfg[\"seq_len\"] - 1)\n",
    "        self.buffer_size = self.buffer_batches * (cfg[\"seq_len\"] - 1)\n",
    "        self.buffer = torch.zeros(\n",
    "            (self.buffer_size, 2, model_A.cfg.d_model),\n",
    "            dtype=torch.float32, #changed from bfloat16 to float32\n",
    "            requires_grad=False,\n",
    "        ).to(cfg[\"device\"]) # hardcoding 2 for model diffing\n",
    "        self.cfg = cfg\n",
    "        self.model_A = model_A\n",
    "        self.model_B = model_B\n",
    "        self.token_pointer = 0\n",
    "        self.first = True\n",
    "        self.normalize = True\n",
    "        self.all_tokens = all_tokens\n",
    "        \n",
    "        estimated_norm_scaling_factor_A = self.estimate_norm_scaling_factor(cfg[\"model_batch_size\"], model_A)\n",
    "        estimated_norm_scaling_factor_B = self.estimate_norm_scaling_factor(cfg[\"model_batch_size\"], model_B)\n",
    "        \n",
    "        self.normalisation_factor = torch.tensor(\n",
    "        [\n",
    "            estimated_norm_scaling_factor_A,\n",
    "            estimated_norm_scaling_factor_B,\n",
    "        ],\n",
    "        device=\"cuda\",\n",
    "        dtype=torch.float32,\n",
    "        )\n",
    "        self.refresh()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def estimate_norm_scaling_factor(self, batch_size, model, n_batches_for_norm_estimate: int = 100):\n",
    "        # stolen from SAELens https://github.com/jbloomAus/SAELens/blob/6d6eaef343fd72add6e26d4c13307643a62c41bf/sae_lens/training/activations_store.py#L370\n",
    "        norms_per_batch = []\n",
    "        for i in tqdm.tqdm(\n",
    "            range(n_batches_for_norm_estimate), desc=\"Estimating norm scaling factor\"\n",
    "        ):\n",
    "            tokens = self.all_tokens[i * batch_size : (i + 1) * batch_size]\n",
    "            _, cache = model.run_with_cache(\n",
    "                tokens,\n",
    "                names_filter=self.cfg[\"hook_point\"],\n",
    "                return_type=None,\n",
    "            )\n",
    "            acts = cache[self.cfg[\"hook_point\"]]\n",
    "            # TODO: maybe drop BOS here\n",
    "            norms_per_batch.append(acts.norm(dim=-1).mean().item())\n",
    "        mean_norm = np.mean(norms_per_batch)\n",
    "        scaling_factor = np.sqrt(model.cfg.d_model) / mean_norm\n",
    "\n",
    "        return scaling_factor\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def refresh(self):\n",
    "        self.pointer = 0\n",
    "        print(\"Refreshing the buffer!\")\n",
    "        with torch.autocast(\"cuda\", torch.bfloat16):\n",
    "            if self.first:\n",
    "                num_batches = self.buffer_batches\n",
    "            else:\n",
    "                num_batches = self.buffer_batches // 2\n",
    "            self.first = False\n",
    "            for _ in tqdm.trange(0, num_batches, self.cfg[\"model_batch_size\"]):\n",
    "                tokens = self.all_tokens[\n",
    "                    self.token_pointer : min(\n",
    "                        self.token_pointer + self.cfg[\"model_batch_size\"], num_batches\n",
    "                    )\n",
    "                ]\n",
    "                _, cache_A = self.model_A.run_with_cache(\n",
    "                    tokens, names_filter=self.cfg[\"hook_point\"]\n",
    "                )\n",
    "                cache_A: ActivationCache\n",
    "\n",
    "                _, cache_B = self.model_B.run_with_cache(\n",
    "                    tokens, names_filter=self.cfg[\"hook_point\"]\n",
    "                )\n",
    "                cache_B: ActivationCache\n",
    "\n",
    "                acts = torch.stack([cache_A[self.cfg[\"hook_point\"]], cache_B[self.cfg[\"hook_point\"]]], dim=0)\n",
    "                acts = acts[:, :, 1:, :] # Drop BOS\n",
    "                assert acts.shape == (2, tokens.shape[0], tokens.shape[1]-1, self.model_A.cfg.d_model) # [2, batch, seq_len, d_model]\n",
    "                acts = einops.rearrange(\n",
    "                    acts,\n",
    "                    \"n_layers batch seq_len d_model -> (batch seq_len) n_layers d_model\",\n",
    "                )\n",
    "\n",
    "                self.buffer[self.pointer : self.pointer + acts.shape[0]] = acts\n",
    "                self.pointer += acts.shape[0]\n",
    "                self.token_pointer += self.cfg[\"model_batch_size\"]\n",
    "\n",
    "        self.pointer = 0\n",
    "        self.buffer = self.buffer[\n",
    "            torch.randperm(self.buffer.shape[0]).to(self.cfg[\"device\"])\n",
    "        ]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def next(self):\n",
    "        out = self.buffer[self.pointer : self.pointer + self.cfg[\"batch_size\"]].float()\n",
    "        # out: [batch_size, n_layers, d_model]\n",
    "        self.pointer += self.cfg[\"batch_size\"]\n",
    "        if self.pointer > self.buffer.shape[0] // 2 - self.cfg[\"batch_size\"]:\n",
    "            self.refresh()\n",
    "        if self.normalize:\n",
    "            out = out * self.normalisation_factor[None, :, None]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "SAVE_DIR = Path(\"./crosscoder-model-diff-replication/checkpoints\")\n",
    "\n",
    "class LossOutput(NamedTuple):\n",
    "    # loss: torch.Tensor\n",
    "    l2_loss: torch.Tensor\n",
    "    l1_loss: torch.Tensor\n",
    "    l0_loss: torch.Tensor\n",
    "    explained_variance: torch.Tensor\n",
    "    explained_variance_A: torch.Tensor\n",
    "    explained_variance_B: torch.Tensor\n",
    "\n",
    "class CrossCoder(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        d_hidden = self.cfg[\"dict_size\"]\n",
    "        d_in = self.cfg[\"d_in\"]\n",
    "        self.dtype = DTYPES[self.cfg[\"enc_dtype\"]]\n",
    "        torch.manual_seed(self.cfg[\"seed\"])\n",
    "        # hardcoding n_models to 2\n",
    "        self.W_enc = nn.Parameter(\n",
    "            torch.empty(2, d_in, d_hidden, dtype=self.dtype)\n",
    "        )\n",
    "        self.W_dec = nn.Parameter(\n",
    "            torch.nn.init.normal_(\n",
    "                torch.empty(\n",
    "                    d_hidden, 2, d_in, dtype=self.dtype\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        self.W_dec = nn.Parameter(\n",
    "            torch.nn.init.normal_(\n",
    "                torch.empty(\n",
    "                    d_hidden, 2, d_in, dtype=self.dtype\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        # Make norm of W_dec 0.1 for each column, separate per layer\n",
    "        with torch.no_grad():\n",
    "            self.W_dec.copy_(self.W_dec / self.W_dec.norm(dim=-1, keepdim=True) * self.cfg[\"dec_init_norm\"])\n",
    "\n",
    "        # Initialise W_enc to be the transpose of W_dec\n",
    "        with torch.no_grad():\n",
    "            self.W_enc.copy_(\n",
    "                einops.rearrange(\n",
    "                    self.W_dec,  # use the parameter directly, not .data\n",
    "                    \"d_hidden n_models d_model -> n_models d_model d_hidden\"\n",
    "                )\n",
    "            )\n",
    "        self.b_enc = nn.Parameter(torch.zeros(d_hidden, dtype=self.dtype))\n",
    "        self.b_dec = nn.Parameter(\n",
    "            torch.zeros((2, d_in), dtype=self.dtype)\n",
    "        )\n",
    "        self.d_hidden = d_hidden\n",
    "\n",
    "        self.to(self.cfg[\"device\"])\n",
    "        self.save_dir = None\n",
    "        self.save_version = 0\n",
    "\n",
    "        for name, param in self.named_parameters():\n",
    "            print(name, param.requires_grad)\n",
    "\n",
    "\n",
    "    def encode(self, x, apply_relu=True):\n",
    "        #x: [batch, n_models, d_model]\n",
    "        #Using torch.einsum instead of einops.einsum\n",
    "        #batch n_models d_model, n_models d_moddel d_hidden, batch d_hidden\n",
    "        x_enc = torch.einsum(\"bnd,ndh->bh\", x, self.W_enc)\n",
    "        if apply_relu:\n",
    "            acts = F.relu(x_enc + self.b_enc)\n",
    "        else:\n",
    "            acts = x_enc + self.b_enc\n",
    "        print(\"Inside encode, x_enc.requires_grad:\", x_enc.requires_grad)\n",
    "        return acts\n",
    "\n",
    "\n",
    "    def decode(self, acts):\n",
    "        # acts: [batch, d_hidden]\n",
    "        #torch.einsum i.o. einops\n",
    "        #batch d_hidden, d_hidden n_model d_model, batch n_model d_model\n",
    "        acts_dec = torch.einsum(\"bh,hnd->bnd\", acts, self.W_dec)\n",
    "        return acts_dec + self.b_dec\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, n_models, d_model]\n",
    "        acts = self.encode(x)\n",
    "        return self.decode(acts)\n",
    "\n",
    "    def get_losses(self, x):\n",
    "        # x: [batch, n_models, d_model]\n",
    "        x = x.to(self.dtype)\n",
    "        print(\"x after conversion requires grad:\", x.requires_grad)\n",
    "        acts = self.encode(x)\n",
    "        # acts: [batch, d_hidden]\n",
    "        print(\"acts after encode requires grad:\", acts.requires_grad)\n",
    "\n",
    "        x_reconstruct = self.decode(acts)\n",
    "        print(\"x_reconstruct after conversion requires grad:\", x_reconstruct.requires_grad)\n",
    "\n",
    "        diff = x_reconstruct.float() - x.float()\n",
    "        print(\"diff after conversion requires grad:\", diff.requires_grad)\n",
    "\n",
    "        squared_diff = diff.pow(2)\n",
    "        l2_per_batch = einops.reduce(squared_diff, 'batch n_models d_model -> batch', 'sum')\n",
    "        l2_loss = l2_per_batch.mean()\n",
    "        print(\"l2 grad\", l2_loss.requires_grad)\n",
    "\n",
    "        total_variance = einops.reduce((x - x.mean(0)).pow(2), 'batch n_models d_model -> batch', 'sum')\n",
    "        explained_variance = 1 - l2_per_batch / total_variance\n",
    "\n",
    "        per_token_l2_loss_A = (x_reconstruct[:, 0, :] - x[:, 0, :]).pow(2).sum(dim=-1).squeeze()\n",
    "        total_variance_A = (x[:, 0, :] - x[:, 0, :].mean(0)).pow(2).sum(-1).squeeze()\n",
    "        explained_variance_A = 1 - per_token_l2_loss_A / total_variance_A\n",
    "\n",
    "        per_token_l2_loss_B = (x_reconstruct[:, 1, :] - x[:, 1, :]).pow(2).sum(dim=-1).squeeze()\n",
    "        total_variance_B = (x[:, 1, :] - x[:, 1, :].mean(0)).pow(2).sum(-1).squeeze()\n",
    "        explained_variance_B = 1 - per_token_l2_loss_B / total_variance_B\n",
    "\n",
    "        decoder_norms = self.W_dec.norm(dim=-1)\n",
    "        # decoder_norms: [d_hidden, n_models]\n",
    "        total_decoder_norm = einops.reduce(decoder_norms, 'd_hidden n_models -> d_hidden', 'sum')\n",
    "        l1_loss = (acts * total_decoder_norm[None, :]).sum(-1).mean(0)\n",
    "        print(\"l1 grad\", l1_loss.requires_grad)\n",
    "\n",
    "\n",
    "        l0_loss = (acts>0).float().sum(-1).mean()\n",
    "        print(\"l0 grad\", l0_loss.requires_grad)\n",
    "\n",
    "\n",
    "        return LossOutput(l2_loss=l2_loss, l1_loss=l1_loss, l0_loss=l0_loss, explained_variance=explained_variance, explained_variance_A=explained_variance_A, explained_variance_B=explained_variance_B)\n",
    "\n",
    "    def create_save_dir(self):\n",
    "        base_dir = Path(\"./crosscoder-model-diff-replication/checkpoints\")\n",
    "        version_list = [\n",
    "            int(file.name.split(\"_\")[1])\n",
    "            for file in list(SAVE_DIR.iterdir())\n",
    "            if \"version\" in str(file)\n",
    "        ]\n",
    "        if len(version_list):\n",
    "            version = 1 + max(version_list)\n",
    "        else:\n",
    "            version = 0\n",
    "        self.save_dir = base_dir / f\"version_{version}\"\n",
    "        self.save_dir.mkdir(parents=True)\n",
    "\n",
    "    def save(self):\n",
    "        if self.save_dir is None:\n",
    "            self.create_save_dir()\n",
    "        weight_path = self.save_dir / f\"{self.save_version}.pt\"\n",
    "        cfg_path = self.save_dir / f\"{self.save_version}_cfg.json\"\n",
    "\n",
    "        torch.save(self.state_dict(), weight_path)\n",
    "        with open(cfg_path, \"w\") as f:\n",
    "            json.dump(self.cfg, f)\n",
    "\n",
    "        print(f\"Saved as version {self.save_version} in {self.save_dir}\")\n",
    "        self.save_version += 1\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def load_from_hf(\n",
    "        cls,\n",
    "        repo_id: str = \"LheaB/crosscoders_tryout\",\n",
    "        path: str = \"blocks.14.hook_resid_pre\",\n",
    "        device: Optional[Union[str, torch.device]] = None\n",
    "    ) -> \"CrossCoder\":\n",
    "        \"\"\"\n",
    "        Load CrossCoder weights and config from HuggingFace.\n",
    "\n",
    "        Args:\n",
    "            repo_id: HuggingFace repository ID\n",
    "            path: Path within the repo to the weights/config\n",
    "            model: The transformer model instance needed for initialization\n",
    "            device: Device to load the model to (defaults to cfg device if not specified)\n",
    "\n",
    "        Returns:\n",
    "            Initialized CrossCoder instance\n",
    "        \"\"\"\n",
    "\n",
    "        # Download config and weights\n",
    "        config_path = hf_hub_download(\n",
    "            repo_id=repo_id,\n",
    "            filename=f\"{path}/cfg.json\"\n",
    "        )\n",
    "        weights_path = hf_hub_download(\n",
    "            repo_id=repo_id,\n",
    "            filename=f\"{path}/cc_weights.pt\"\n",
    "        )\n",
    "\n",
    "        # Load config\n",
    "        with open(config_path, 'r') as f:\n",
    "            cfg = json.load(f)\n",
    "\n",
    "        # Override device if specified\n",
    "        if device is not None:\n",
    "            cfg[\"device\"] = str(device)\n",
    "\n",
    "        # Initialize CrossCoder with config\n",
    "        instance = cls(cfg)\n",
    "\n",
    "        # Load weights\n",
    "        state_dict = torch.load(weights_path, map_location=cfg[\"device\"])\n",
    "        instance.load_state_dict(state_dict)\n",
    "\n",
    "        return instance\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, version_dir, checkpoint_version):\n",
    "        save_dir = Path(\"/workspace/crosscoder-model-diff-replication/checkpoints\") / str(version_dir)\n",
    "        cfg_path = save_dir / f\"{str(checkpoint_version)}_cfg.json\"\n",
    "        weight_path = save_dir / f\"{str(checkpoint_version)}.pt\"\n",
    "\n",
    "        cfg = json.load(open(cfg_path, \"r\"))\n",
    "        pprint.pprint(cfg)\n",
    "        self = cls(cfg=cfg)\n",
    "        self.load_state_dict(torch.load(weight_path))\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, cfg, model_A, model_B, all_tokens):\n",
    "        self.cfg = cfg\n",
    "        self.model_A = model_A\n",
    "        self.model_B = model_B\n",
    "        self.crosscoder = CrossCoder(cfg)\n",
    "        self.buffer = Buffer(cfg, model_A, model_B, all_tokens)\n",
    "        self.total_steps = cfg[\"num_tokens\"] // cfg[\"batch_size\"]\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.crosscoder.parameters(),\n",
    "            lr=cfg[\"lr\"],\n",
    "            betas=(cfg[\"beta1\"], cfg[\"beta2\"]),\n",
    "        )\n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "            self.optimizer, self.lr_lambda\n",
    "        )\n",
    "        self.step_counter = 0\n",
    "\n",
    "        wandb.init(project=cfg[\"wandb_project\"], entity=cfg[\"wandb_entity\"])\n",
    "\n",
    "    def lr_lambda(self, step):\n",
    "        if step < 0.8 * self.total_steps:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 1.0 - (step - 0.8 * self.total_steps) / (0.2 * self.total_steps)\n",
    "\n",
    "    def get_l1_coeff(self):\n",
    "        # Linearly increases from 0 to cfg[\"l1_coeff\"] over the first 0.05 * self.total_steps steps, then keeps it constant\n",
    "        if self.step_counter < 0.05 * self.total_steps:\n",
    "            return self.cfg[\"l1_coeff\"] * self.step_counter / (0.05 * self.total_steps)\n",
    "        else:\n",
    "            return self.cfg[\"l1_coeff\"]\n",
    "\n",
    "    def step(self):\n",
    "        #acts = self.buffer.next()\n",
    "        acts = self.buffer.next().clone().requires_grad_()\n",
    "        losses = self.crosscoder.get_losses(acts)\n",
    "        loss = losses.l2_loss + self.get_l1_coeff() * losses.l1_loss\n",
    "        print(\"Loss requires grad:\", loss.requires_grad)\n",
    "        loss.backward()  # Should work if everything is connected\n",
    "\n",
    "        clip_grad_norm_(self.crosscoder.parameters(), max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        loss_dict = {\n",
    "            \"loss\": loss.item(),\n",
    "            \"l2_loss\": losses.l2_loss.item(),\n",
    "            \"l1_loss\": losses.l1_loss.item(),\n",
    "            \"l0_loss\": losses.l0_loss.item(),\n",
    "            \"l1_coeff\": self.get_l1_coeff(),\n",
    "            \"lr\": self.scheduler.get_last_lr()[0],\n",
    "            \"explained_variance\": losses.explained_variance.mean().item(),\n",
    "            \"explained_variance_A\": losses.explained_variance_A.mean().item(),\n",
    "            \"explained_variance_B\": losses.explained_variance_B.mean().item(),\n",
    "        }\n",
    "        self.step_counter += 1\n",
    "        return loss_dict\n",
    "\n",
    "    def log(self, loss_dict):\n",
    "        wandb.log(loss_dict, step=self.step_counter)\n",
    "        print(loss_dict)\n",
    "\n",
    "    def save(self):\n",
    "        self.crosscoder.save()\n",
    "\n",
    "    def train(self):\n",
    "        self.step_counter = 0\n",
    "        try:\n",
    "            for i in tqdm.trange(self.total_steps):\n",
    "                loss_dict = self.step()\n",
    "                if i % self.cfg[\"log_every\"] == 0:\n",
    "                    self.log(loss_dict)\n",
    "                if (i + 1) % self.cfg[\"save_every\"] == 0:\n",
    "                    self.save()\n",
    "        finally:\n",
    "            self.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting conversion to tensor...\n",
      "Conversion complete.\n",
      "torch.Size([114248, 1024])\n"
     ]
    }
   ],
   "source": [
    "def load_wikitext_103_tokenized(block_size=1024):\n",
    "    # Load the raw WikiText-103 dataset (raw version for unprocessed text)\n",
    "    dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"train\")\n",
    "    \n",
    "    # Initialize the GPT2 tokenizer (matching your model's tokenizer)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "    \n",
    "    # Tokenize the text; add special tokens if needed\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], add_special_tokens=True)\n",
    "    \n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "    \n",
    "    # Group all tokens into sequences of length block_size\n",
    "    def group_texts(examples):\n",
    "        # Concatenate all token lists into a single list\n",
    "        concatenated_input_ids = sum(examples['input_ids'], [])\n",
    "        concatenated_attention_mask = sum(examples[\"attention_mask\"], [])\n",
    "        total_length = len(concatenated_input_ids)\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "        input_ids = [concatenated_input_ids[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        attention_masks = [concatenated_attention_mask[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_masks}\n",
    "    \n",
    "    grouped_dataset = tokenized_dataset.map(group_texts, batched=True)\n",
    "    \n",
    "    # Convert the grouped token lists into a tensor\n",
    "    print(\"Starting conversion to tensor...\", flush=True)\n",
    "    all_tokens = torch.tensor(grouped_dataset[\"input_ids\"])\n",
    "    print(\"Conversion complete.\")\n",
    "    print(all_tokens.shape)\n",
    "    return all_tokens\n",
    "\n",
    "# Load tokens from WikiText-103\n",
    "all_tokens = load_wikitext_103_tokenized(block_size=1024)\n",
    "\n",
    "def arg_parse_update_cfg(default_cfg):\n",
    "    \"\"\"\n",
    "    Helper function to take in a dictionary of arguments, convert these to command line arguments, look at what was passed in, and return an updated dictionary.\n",
    "\n",
    "    If in Ipython, just returns with no changes\n",
    "    \"\"\"\n",
    "    if get_ipython() is not None:\n",
    "        # Is in IPython\n",
    "        print(\"In IPython - skipped argparse\")\n",
    "        return default_cfg\n",
    "    cfg = dict(default_cfg)\n",
    "    parser = argparse.ArgumentParser()\n",
    "    for key, value in default_cfg.items():\n",
    "        if type(value) == bool:\n",
    "            # argparse for Booleans is broken rip. Now you put in a flag to change the default --{flag} to set True, --{flag} to set False\n",
    "            if value:\n",
    "                parser.add_argument(f\"--{key}\", action=\"store_false\")\n",
    "            else:\n",
    "                parser.add_argument(f\"--{key}\", action=\"store_true\")\n",
    "\n",
    "        else:\n",
    "            parser.add_argument(f\"--{key}\", type=type(value), default=value)\n",
    "    args = parser.parse_args()\n",
    "    parsed_args = vars(args)\n",
    "    cfg.update(parsed_args)\n",
    "    print(\"Updated config\")\n",
    "    print(json.dumps(cfg, indent=2))\n",
    "    return cfg \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In IPython - skipped argparse\n"
     ]
    }
   ],
   "source": [
    "\n",
    "default_cfg = {\n",
    "    \"seed\": 49,\n",
    "    \"batch_size\": 512,\n",
    "    \"buffer_mult\": 16,\n",
    "    \"lr\": 5e-5,\n",
    "    \"num_tokens\": 100_000,\n",
    "    \"l1_coeff\": 2,\n",
    "    \"beta1\": 0.9,\n",
    "    \"beta2\": 0.999,\n",
    "    \"d_in\": base_model.cfg.d_model,\n",
    "    \"dict_size\": 2**14,\n",
    "    \"seq_len\": 1024,\n",
    "    \"enc_dtype\": \"fp32\",\n",
    "    \"model_name\": \"gpt2-medium\",\n",
    "    \"site\": \"resid_pre\",\n",
    "    \"device\": \"cuda\",\n",
    "    \"model_batch_size\": 3,\n",
    "    \"log_every\": 100,\n",
    "    \"save_every\": 30000,\n",
    "    \"dec_init_norm\": 0.64,\n",
    "    \"hook_point\": \"blocks.14.hook_resid_pre\",\n",
    "    \"wandb_project\": \"crosscoders_tryout\",\n",
    "    \"wandb_entity\": \"lhealhea-eth-z-rich\"\n",
    "    }\n",
    "\n",
    "cfg = arg_parse_update_cfg(default_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_enc True\n",
      "W_dec True\n",
      "b_enc True\n",
      "b_dec True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estimating norm scaling factor: 100%|██████████| 100/100 [00:43<00:00,  2.30it/s]\n",
      "Estimating norm scaling factor: 100%|██████████| 100/100 [00:43<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:02<00:00,  1.18it/s]\n",
      "  0%|          | 0/195 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x after conversion requires grad: True\n",
      "self weight True\n",
      "x grad True\n",
      "Inside encode, x_enc.requires_grad: False\n",
      "acts after encode requires grad: False\n",
      "x_reconstruct after conversion requires grad: False\n",
      "diff after conversion requires grad: False\n",
      "l2 grad False\n",
      "l1 grad False\n",
      "l0 grad False\n",
      "Loss requires grad: False\n",
      "Saved as version 0 in crosscoder-model-diff-replication/checkpoints/version_9\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(cfg, base_model, chat_model, all_tokens)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 73\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtrange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_steps):\n\u001b[0;32m---> 73\u001b[0m         loss_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_every\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     75\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(loss_dict)\n",
      "Cell \u001b[0;32mIn[6], line 41\u001b[0m, in \u001b[0;36mTrainer.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m loss \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39ml2_loss \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_l1_coeff() \u001b[38;5;241m*\u001b[39m losses\u001b[38;5;241m.\u001b[39ml1_loss\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss requires grad:\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[0;32m---> 41\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Should work if everything is connected\u001b[39;00m\n\u001b[1;32m     43\u001b[0m clip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrosscoder\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(cfg, base_model, chat_model, all_tokens)\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

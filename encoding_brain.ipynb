{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data_fns = sorted(glob('brainiak-aperture-srm-data/sub-*_task-black_*bold.nii.gz'))\n",
    "atlas_fn = 'brainiak-aperture-srm-data/Schaefer2018_400Parcels_17Networks_order_FSLMNI152_2.5mm.nii.gz'\n",
    "\n",
    "# Load in the Schaefer 400-parcel atlas\n",
    "atlas_nii = nib.load(atlas_fn)\n",
    "atlas_img = atlas_nii.get_fdata()\n",
    "\n",
    "# Left temporal parietal ROI labels\n",
    "parcel_labels = [195, 196, 197, 198, 199, 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in functional data and mask with \"temporal parietal\" ROI\n",
    "data = []\n",
    "for data_fn in data_fns:\n",
    "    voxel_data = nib.load(data_fn).get_fdata()\n",
    "\n",
    "    # Take union of all parcels (brain areas) comprising the full ROI\n",
    "    roi_data = np.column_stack([voxel_data[atlas_img == parcel, :].T\n",
    "                                for parcel in parcel_labels])\n",
    "    data.append(roi_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import plot_stat_map\n",
    "\n",
    "# Visualize the left temporal parietal ROI\n",
    "sns.set(palette='colorblind')\n",
    "roi_img = np.zeros(atlas_img.shape)\n",
    "for parcel in parcel_labels:\n",
    "    roi_img[atlas_img == parcel] = 1\n",
    "\n",
    "# Convert to a NIfTI image for visualization with Nilearn\n",
    "roi_nii = nib.Nifti1Image(roi_img, atlas_nii.affine, atlas_nii.header)\n",
    "\n",
    "# Plot plot left temporal parietal ROI\n",
    "plot_stat_map(roi_nii, cmap='tab10_r', cut_coords=(-53, -46, 10),\n",
    "              colorbar=False, title='left temporal parietal ROI');\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#participants\n",
    "print(len(data))\n",
    "#time, voxels\n",
    "print(roi_data.shape)\n",
    "#X, Y, Z, time\n",
    "print(voxel_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, subject_data in enumerate(data):\n",
    "    print(f\"Subject {i} training data shape: {subject_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "class FMRIAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim=935, hidden_dim=1024, latent_dim=768):\n",
    "        super(FMRIAutoencoder, self).__init__()\n",
    "        # Encoder: maps fmri input to latent space\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim)\n",
    "        )\n",
    "        # Decoder: reconstructs fmri input from latent representation\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        reconstruction = self.decoder(latent)\n",
    "        return latent, reconstruction\n",
    "\n",
    "class FMRI_Dataset(Dataset):\n",
    "    def __init__(self, fmri_list):\n",
    "        # Concatenate along the first axis: all timepoints from all subjects\n",
    "        self.data = np.concatenate(fmri_list, axis=0)  # Shape: (550 * num_subjects, 935)\n",
    "        self.data = torch.tensor(self.data, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "        \n",
    "dataset = FMRI_Dataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "model = FMRIAutoencoder(input_dim=935, hidden_dim=1024, latent_dim=768)\n",
    "\n",
    "# Define the reconstruction loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 20  # Adjust\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for fmri_sample in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        latent, recon = model(fmri_sample)\n",
    "        loss = criterion(recon, fmri_sample)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(dataloader)}\")\n",
    "\n",
    "# Extract the encoder\n",
    "fmri_encoder = model.encoder\n",
    "\n",
    "# Save the encoder's weights\n",
    "torch.save(fmri_encoder.state_dict(), \"fmri_encoder_weights.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# At the end of encoding_brain.ipynb\n",
    "__all__ = ['FMRIAutoencoder']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
